{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BATCH_START =0\n",
    "BATCH_SIZE = 50   # 一個step丟多少資料（自設）\n",
    "BATCH_START_TEST = 0\n",
    "\n",
    "TIME_STEPS = 20   #跑幾次step\n",
    "\n",
    "INPUT_SIZE = 1\n",
    "OUTPUT_SIZE = 1\n",
    "\n",
    "CELL_SIZE = 10   #神經元數量（自設）\n",
    "LR = 0.006\n",
    "\n",
    "#產生數據\n",
    "def get_batch():\n",
    "    global BATCH_START,TIME_STEPS\n",
    "    # xs_shape = [BATCH_SIZE, TIME_STEPS] =[50, 20] ，並生成整個矩陣大小的數據\n",
    "    xs = np.arange(BATCH_START, BATCH_START + BATCH_SIZE*TIME_STEPS).reshape((BATCH_SIZE, TIME_STEPS))/(10*np.pi)\n",
    "    # /(10*np.pi)是為了normalization\n",
    "    \n",
    "    seq = np.sin(xs)  #sequence 順序\n",
    "    res = np.con(xs)  #result  要求\n",
    "    \n",
    "    BATCH_START += BATCH_STEPS  #隨時間推移，所以下一個step的生成的資料起始跟結束就不一樣\n",
    "    \n",
    "    ##畫圖的部分\n",
    "    ##plt.plot(xs[0, :], seq[0, :], \"r\")\n",
    "    ##plt.plot(xs[0, :], res[0, :], \"b--\")\n",
    "    ##plt.show()\n",
    "    \n",
    "    #np.newaxis 多加一個維度\n",
    "    #returned seq_shape(batch_size, time_step, input_size) \n",
    "    #returned res_shape(batch_size, time_step, input_size) \n",
    "    #returned xs_shape(batch_size, time_step) \n",
    "    return [seq[:, :, np.newaxis], res[:, :, np.newaxis], xs] \n",
    "\n",
    "########################## 以上目前理解 ##############################\n",
    "################## 以下 重點 ###################\n",
    "\n",
    "#建立LSTM \n",
    "class LSTM_RNN(object):\n",
    "    def __init__(self, n_steps, input_size, output_size, cell_size, batch_size):\n",
    "        self.n_steps = n_steps\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.cell_size = cell_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        with tf.name_scope('inputs'):\n",
    "            self.xs = tf.placeholder(tf.float32, [None, n_steps, input_size], name='xs')\n",
    "            self.ys = tf.placeholder(tf.float32, [None, n_steps, output_size], name='ys')\n",
    "        \n",
    "        with tf.variable_scope('in_hidden'):\n",
    "            self.add_input_layer()            \n",
    "        with tf.variable_scope('LSTM_cell'):\n",
    "            self.add_cell()            \n",
    "        with tf.variable_scope('out_hidden'):\n",
    "            self.add_output_layer()\n",
    "        \n",
    "        with tf.name_scope('cost'):\n",
    "            self.compute_cost()\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(LR).minimize(self.cost)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #定義input_layer(隱藏層)\n",
    "    def add_input_layer(self):\n",
    "        #xs三維轉二維，為了跟w（二維）相乘\n",
    "        l_in_x = tf.reshape(self.xs, [-1, self.input_size], name=\"to_2D\")  #l_in_x_shape=[batch*n_step, input_size]\n",
    "        \n",
    "        Ws_in = self._weight_variable([self.input_size, self.cell_size])\n",
    "        bs_in = self._bias_variable([self.cell_size,])\n",
    "        \n",
    "        #Wx+b\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            to_cell_x = tf.matmul(l_in_x, Ws_in) + bs_in\n",
    "            \n",
    "        #xs二維再轉三維，準備丟進cell   \n",
    "        self.to_cell_x = tf.reshape(to_cell_x, [-1, self.n_steps, self.cell_size], name='to_3D')\n",
    "      \n",
    "    #定義cell    \n",
    "    def add_cell(self):\n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(self.cell_size, forget_bias=1.0, state_is_tuple=True)\n",
    "        with tf.name_scope('initial_state'):\n",
    "            self.cell_init_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "        self.cell_outputs, self.cell_final_state = tf.nn.dynamic_rnn(lstm_cell, \n",
    "                                                                     self.to_cell_x, \n",
    "                                                                     initial_state=self.cell_init_state, \n",
    "                                                                     time_major=False)\n",
    "    \n",
    "    #定義output_layer\n",
    "    def add_output_layer(self):\n",
    "        #xs三維轉二維，為了跟w（二維）相乘\n",
    "        l_out_x = tf.reshape(self.cell_outputs, [-1, self.cell_size], name='to_2D')\n",
    "        \n",
    "        Ws_out = self._weight_variable([self.cell_size, self.output_size])\n",
    "        bs_out = self._bias_variable([self.output_size, ])\n",
    "        \n",
    "        #Wx+b\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            self.pred = tf.matmul(l_out_x, Ws_out) + bs_out\n",
    "    \n",
    "    #計算loss, cost\n",
    "    def compute_cost(self):\n",
    "        #每次step的loss\n",
    "        losses = tf.nn.seq2seq.sequence_loss_by_example([tf.reshape(self.pred, [-1], name='reshape_pred')], \n",
    "                                                        [tf.reshape(self.ys, [-1], name='reshape_target')], \n",
    "                                                      \n",
    "                                                        [tf.ones([self.batch_size * self.n_steps], dtype=tf.float32)], \n",
    "                                                        average_across_timesteps=True, \n",
    "                                                        softmax_loss_function=self.ms_error, \n",
    "                                                        name='losses')\n",
    "        \n",
    "        with tf.name_scope('average_cost'):\n",
    "            self.cost = tf.div(tf.reduce_sum(losses, name='losses_sum'), \n",
    "                               self.batch_size,\n",
    "                               name='average_cost')\n",
    "            \n",
    "            tf.summary.scalar('cost', self.cost)\n",
    "    \n",
    "    def ms_error(self, y_pre, y_target):\n",
    "        return tf.square(tf.sub(y_pre, y_target))\n",
    "    \n",
    "    \n",
    "    def _weight_variable(self, shape, name='weights'):\n",
    "        initializer = tf.random_normal_initializer(mean=0., stddev=1.)\n",
    "        return tf.get_variable(shape=shape, initializer=initializer, name=name)\n",
    "    \n",
    "    def _bias_variable(self, shape, name='biases'):\n",
    "        initializer = tf.constant_initializer(0.1)\n",
    "        return tf.get_variable(shape=shape, initializer=initializer, name=name)\n",
    "    \n",
    "    \n",
    "################## 以上 重點 ###################\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    model = LSTM_RNN(TIME_STEPS, INPUT_SIZE, OUTPUT_SIZE, CELL_SIZE, BATCH_SIZE)\n",
    "    sess = tf.Session()\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"logs\", sess.graph)\n",
    "    # tf.initialize_all_variables() no long valid from\n",
    "    # 2017-03-02 if using tensorflow >= 0.12\n",
    "    if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
    "        init = tf.initialize_all_variables()\n",
    "    else:\n",
    "        init = tf.global_variables_initializer()\n",
    "    sess.run(init)        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* returned seq, res, xs   :shape(batch, step, input)   \n",
    "為什麼代近sin就是batch? con是step?  \n",
    "\n",
    "* return [seq[:, :, np.newaxis], res[:, :, np.newaxis], xs]  \n",
    "而這裡又為何又用一個列表？\n",
    "\n",
    "現在暫時能理解seq,res多加一個維度是像“第一頁”“第二頁”的概念（隨時間推移）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 可視化 \n",
    "在目的資料夾開啟terminel  \n",
    "指令： tensorboard --logdir='logs'  \n",
    "會跑出 網址： 例：(You can navigate to http://192.168.1.52:6006)  \n",
    "\n",
    "![tensorboard](https://dl.dropboxusercontent.com/u/23064459/python/2017-01%20%E5%91%A8%E8%8E%AB%E7%85%A9%20tansorflow%20%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E5%AD%B8%E7%BF%92/tensorboard.png)\n",
    "![tensorboard](https://dl.dropboxusercontent.com/u/23064459/python/2017-01%20%E5%91%A8%E8%8E%AB%E7%85%A9%20tansorflow%20%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E5%AD%B8%E7%BF%92/tensorboard-2.png)\n",
    "![tensorboard](https://dl.dropboxusercontent.com/u/23064459/python/2017-01%20%E5%91%A8%E8%8E%AB%E7%85%A9%20tansorflow%20%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E5%AD%B8%E7%BF%92/tensorboard-3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
